{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe0b6c28-2593-4f12-abf1-b07390565387",
   "metadata": {},
   "source": [
    "# Loading the raw dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e9c5864-ceae-4152-81f0-7e3c225cdc85",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"json\", data_files=\"./datasets/raw.jsonl\", split=\"train\")\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c76e09d-070e-410c-87fc-df81af726106",
   "metadata": {},
   "source": [
    "# Filtering the short abstracts (< 30 words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2922b55f-eb33-4814-956e-10a2179e8605",
   "metadata": {},
   "outputs": [],
   "source": [
    "def abstract_has_30_words(x: dict) -> bool:\n",
    "  if (len(x[\"abstract\"]) == 0):\n",
    "    return False\n",
    "  \n",
    "  words = \" \".join(x[\"abstract\"]).split()\n",
    "\n",
    "  return len(words) >= 30\n",
    "\n",
    "dataset = dataset.filter(abstract_has_30_words)\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9cf1a49-a77e-44a6-8923-1a5d1ee63dc0",
   "metadata": {},
   "source": [
    "# Randomly sample 5 balanced datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c19830c-2fb1-412b-b4cc-495812830d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import DatasetDict, Dataset, concatenate_datasets\n",
    "\n",
    "# helper function to split a given number (n: int) or by a given % (n: float).\n",
    "def split_dataset(d: Dataset, n: int|float, seed = 42) -> tuple[Dataset, Dataset]:\n",
    "  splitted = d.train_test_split(n, seed=seed)\n",
    "  return splitted[\"test\"], splitted[\"train\"]\n",
    "\n",
    "# filters for each label.\n",
    "hh_pos_filter = lambda x: x[\"type\"] == \"hh\" and x[\"is_selected\"]\n",
    "vh_pos_filter = lambda x: x[\"type\"] == \"vh\" and x[\"is_selected\"]\n",
    "xx_neg_filter = lambda x: not x[\"is_selected\"]\n",
    "\n",
    "def sample_balanced_datasets(dataset: Dataset, n = 5, test_size: int|float = 0.1, seed = 42) -> list[DatasetDict]:\n",
    "  # get the examples of each label.\n",
    "  hh_pos = dataset.filter(hh_pos_filter)\n",
    "  vh_pos = dataset.filter(vh_pos_filter)\n",
    "  xx_neg = dataset.filter(xx_neg_filter)\n",
    "\n",
    "  # get 10% of positive examples for test dataset.\n",
    "  hh_pos_test, hh_pos_tmp = split_dataset(hh_pos, test_size, seed)\n",
    "  vh_pos_test, vh_pos_tmp = split_dataset(vh_pos, test_size, seed)\n",
    "\n",
    "  # get the numbers of positive examples.\n",
    "  hh_pos_test_num = len(hh_pos_test)\n",
    "  vh_pos_test_num = len(vh_pos_test)\n",
    "  xx_pos_test_num = hh_pos_test_num + vh_pos_test_num\n",
    "\n",
    "  # get as much negative examples as positive test examples.\n",
    "  xx_neg_test, xx_neg_tmp = split_dataset(xx_neg, xx_pos_test_num, seed)\n",
    "\n",
    "  # create the test dataset.\n",
    "  test_dataset = concatenate_datasets([hh_pos_test, vh_pos_test, xx_neg_test]).shuffle(seed=seed) # (should be shuffled so data is not ordered by labels?)\n",
    "\n",
    "  # now sample 5 times 10% of tmp datasets for train/validation datasets.\n",
    "  datasets = []\n",
    "\n",
    "  for i in range(5):\n",
    "    hh_pos_validation, hh_pos_train = split_dataset(hh_pos_tmp, hh_pos_test_num, seed + i)\n",
    "    vh_pos_validation, vh_pos_train = split_dataset(vh_pos_tmp, vh_pos_test_num, seed + i)\n",
    "\n",
    "    xx_pos_train_num = len(hh_pos_train) + len(vh_pos_train)\n",
    "\n",
    "    xx_neg_train, xx_neg_tmp = split_dataset(xx_neg_tmp, xx_pos_train_num, seed + i)\n",
    "    xx_neg_validation, _ = split_dataset(xx_neg_tmp, xx_pos_test_num, seed + i)\n",
    "\n",
    "    datasets.append(DatasetDict({\n",
    "      \"train\": concatenate_datasets([hh_pos_train, vh_pos_train, xx_neg_train]).shuffle(seed=seed + i),\n",
    "      \"validation\": concatenate_datasets([hh_pos_validation, vh_pos_validation, xx_neg_validation]).shuffle(seed=seed + i),\n",
    "      \"test\": test_dataset,\n",
    "    }))\n",
    "\n",
    "  return datasets\n",
    "\n",
    "# sample 5 balanced datasets.\n",
    "datasets = sample_balanced_datasets(dataset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "433c9ae1",
   "metadata": {},
   "source": [
    "# Inspect the 5 balanced datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f019285",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspect each datasets.\n",
    "for d in datasets:\n",
    "  len11 = len(d[\"train\"].filter(hh_pos_filter))\n",
    "  len12 = len(d[\"train\"].filter(vh_pos_filter))\n",
    "  len13 = len(d[\"train\"].filter(xx_neg_filter))\n",
    "  slice1 = d[\"train\"].select(range(3))\n",
    "\n",
    "  len21 = len(d[\"validation\"].filter(hh_pos_filter))\n",
    "  len22 = len(d[\"validation\"].filter(vh_pos_filter))\n",
    "  len23 = len(d[\"validation\"].filter(xx_neg_filter))\n",
    "\n",
    "  len31 = len(d[\"test\"].filter(hh_pos_filter))\n",
    "  len32 = len(d[\"test\"].filter(vh_pos_filter))\n",
    "  len33 = len(d[\"test\"].filter(xx_neg_filter))\n",
    "\n",
    "  print(len11, len12, len13, len11 + len12 + len13)\n",
    "  print(d[\"train\"][range(3)])\n",
    "\n",
    "  print(len21, len22, len23, len21 + len22 + len23)\n",
    "  print(d[\"validation\"][range(3)])\n",
    "\n",
    "  print(len31, len32, len33, len31 + len32 + len33)\n",
    "  print(d[\"test\"][range(3)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23159625",
   "metadata": {},
   "source": [
    "# Save all the datasets to disk (full + 5 balanced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ef0b26a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.save_to_disk(\"datasets/dataset_full.hf\")\n",
    "\n",
    "for i, dataset in enumerate(datasets):\n",
    "  dataset.save_to_disk(f\"datasets/dataset_balanced{i + 1}.hf\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
